{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cred_fp = '/ebs_volume/data/Credible/'\n",
    "ncred_fp = '/ebs_volume/data/notCredible/'\n",
    "\n",
    "articles = pd.DataFrame(columns=('label',\n",
    "                                 'text',\n",
    "                                 'title',\n",
    "                                 'date',\n",
    "                                 'source'))\n",
    "i = 0    \n",
    "for root, dirs, files in os.walk(cred_fp):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\") and 'api' not in file:\n",
    "             curr_file = os.path.join(root, file)\n",
    "             #print curr_file\n",
    "             with open(curr_file) as json_file:\n",
    "                try:\n",
    "                    data = json.load(json_file)\n",
    "                    if data[\"source\"] == \"new-york-times\":\n",
    "                        articles.loc[i] = [0,data[\"text\"],data[\"title\"],data[\"date\"],\"the-new-york-times\"]\n",
    "                    else:                        \n",
    "                        articles.loc[i] = [0,data[\"text\"],data[\"title\"],data[\"date\"],data[\"source\"]]\n",
    "                    i+=1\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "for root, dirs, files in os.walk(ncred_fp):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\") and 'api' not in file:\n",
    "             curr_file = os.path.join(root, file)\n",
    "             #print curr_file\n",
    "             with open(curr_file) as json_file:\n",
    "                try:\n",
    "                    data = json.load(json_file)\n",
    "                    articles.loc[i] = [1,data[\"text\"],data[\"title\"],data[\"date\"],data[\"source\"]]\n",
    "                    i+=1\n",
    "                except ValueError:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">0.0</th>\n",
       "      <th>count</th>\n",
       "      <td>2086</td>\n",
       "      <td>2086</td>\n",
       "      <td>2086</td>\n",
       "      <td>2086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>1899</td>\n",
       "      <td>1898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>04-05-2017</td>\n",
       "      <td>the-new-york-times</td>\n",
       "      <td></td>\n",
       "      <td>Article 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>67</td>\n",
       "      <td>221</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1.0</th>\n",
       "      <th>count</th>\n",
       "      <td>4648</td>\n",
       "      <td>4648</td>\n",
       "      <td>4648</td>\n",
       "      <td>4648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>51</td>\n",
       "      <td>14</td>\n",
       "      <td>3758</td>\n",
       "      <td>3873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>02-25-2017</td>\n",
       "      <td>activistpost</td>\n",
       "      <td></td>\n",
       "      <td>John McCain Illegally Travels To Syria, Meets ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>169</td>\n",
       "      <td>695</td>\n",
       "      <td>46</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date              source  text  \\\n",
       "label                                                \n",
       "0.0   count         2086                2086  2086   \n",
       "      unique          44                  11  1899   \n",
       "      top     04-05-2017  the-new-york-times         \n",
       "      freq            67                 221    22   \n",
       "1.0   count         4648                4648  4648   \n",
       "      unique          51                  14  3758   \n",
       "      top     02-25-2017        activistpost         \n",
       "      freq           169                 695    46   \n",
       "\n",
       "                                                          title  \n",
       "label                                                            \n",
       "0.0   count                                                2086  \n",
       "      unique                                               1898  \n",
       "      top                                            Article 50  \n",
       "      freq                                                    9  \n",
       "1.0   count                                                4648  \n",
       "      unique                                               3873  \n",
       "      top     John McCain Illegally Travels To Syria, Meets ...  \n",
       "      freq                                                   11  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6734\n",
      "5656\n"
     ]
    }
   ],
   "source": [
    "#Remove duplicate articles\n",
    "print(len(articles))\n",
    "unique_articles = articles.drop_duplicates(subset = 'text')\n",
    "print(len(unique_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5656\n",
      "5521\n"
     ]
    }
   ],
   "source": [
    "#Remove really short articles (<=200 chars)\n",
    "print(len(unique_articles))\n",
    "unique_articles = unique_articles[unique_articles[\"text\"].str.len()>200]\n",
    "print(len(unique_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ItMakesSenseBlog': 152,\n",
       "         'activistpost': 362,\n",
       "         'bbc-news': 206,\n",
       "         'bostonglobe': 111,\n",
       "         'darkmoon': 14,\n",
       "         'dcclothesline': 432,\n",
       "         'empirenews': 13,\n",
       "         'gopthedailydose': 546,\n",
       "         'independent': 215,\n",
       "         'infostormer': 152,\n",
       "         'latimes': 151,\n",
       "         'national-geographic': 171,\n",
       "         'nature': 20,\n",
       "         'reuters': 211,\n",
       "         'rickwells': 352,\n",
       "         'success-street': 299,\n",
       "         'the-new-york-times': 151,\n",
       "         'the-wall-street-journal': 215,\n",
       "         'the-washington-post': 218,\n",
       "         'usa-today': 218,\n",
       "         'usanewsflash': 494,\n",
       "         'usapoliticsnow': 428,\n",
       "         'usasupreme': 323,\n",
       "         'usfanzone': 67})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(unique_articles[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of credible articles: 1013\n",
      "Number of non-credible articles: 3388\n"
     ]
    }
   ],
   "source": [
    "#cred_articles = unique_articles[unique_articles[\"label\"]==0.0]\n",
    "cred_articles = unique_articles[unique_articles[\"source\"].isin([\"new-york-times\",\"the-new-york-times\",\"reuters\",\"the-wall-street-journal\",\"the-washington-post\",\"usa-today\"])]\n",
    "num_cred_articles = len(cred_articles)\n",
    "print(\"Number of credible articles: {}\".format(num_cred_articles))\n",
    "#noncred_articles = unique_articles[unique_articles[\"label\"]==1.0]\n",
    "noncred_articles = unique_articles[unique_articles[\"source\"].isin([\"activistpost\",\"dcclothesline\",\"gopthedailydose\",\"infostormer\",\"rickwells\",\"success-street\",\"usanewsflash\",\"usapoliticsnow\",\"usasupreme\"])]\n",
    "print(\"Number of non-credible articles: {}\".format(len(noncred_articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we desire an even number of credible/non-credible articles in our training set, we will need to downsample our non-credible set. We can sample in a way such that the number of credible and non-credible articles are equal for each day that we've been collecting data. This eliminates the possibility of a temporal bias appearing in our training set by chance occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cred_articles = cred_articles[~cred_articles[\"date\"].isin(list(set(cred_articles[\"date\"]) - set(noncred_articles[\"date\"])))]\n",
    "\n",
    "date_cnts = Counter(cred_articles[\"date\"])\n",
    "noncred_even = pd.DataFrame(columns=('label','text','title','date','source'))\n",
    "\n",
    "for date in date_cnts:\n",
    "    noncred_even = pd.concat([noncred_even, noncred_articles[noncred_articles[\"date\"]==date].sample(n=date_cnts[date])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test on 1930 articles\n"
     ]
    }
   ],
   "source": [
    "even_articles = pd.concat([cred_articles, noncred_even])\n",
    "print(\"Train/Test on {} articles\".format(len(even_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the-washington-post': 208, 'usa-today': 208, 'the-wall-street-journal': 205, 'reuters': 201, 'gopthedailydose': 165, 'the-new-york-times': 143, 'usanewsflash': 139, 'usapoliticsnow': 132, 'dcclothesline': 124, 'activistpost': 106, 'success-street': 93, 'usasupreme': 93, 'rickwells': 86, 'infostormer': 27})\n"
     ]
    }
   ],
   "source": [
    "source_counts = Counter(even_articles[\"source\"])\n",
    "print(source_counts)\n",
    "\n",
    "# plt.bar(range(len(source_counts)), source_counts.values(), align='center')\n",
    "# plt.xticks(range(len(source_counts)), source_counts.keys())\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Content-Based Classifier\n",
    "\n",
    "### 2.0.1) MNB on Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435    Wilbur Ross stands after being sworn in as Sec...\n",
       "436    Samsung Group chief, Jay Y. Lee arrives at the...\n",
       "437    A refugee walks along railway tracks from the ...\n",
       "438    Handout photo provided to Reuters on February ...\n",
       "439    U.S. President Donald Trump addresses Joint Se...\n",
       "440    U.S. President Donald Trump looks up while att...\n",
       "441    U.S. President Donald Trump attends a meeting ...\n",
       "442    Injured people are assisted after an incident ...\n",
       "443    WASHINGTON The Republican chairman of the U.S....\n",
       "444    MILWAUKEE A police officer and three other peo...\n",
       "445    A Texas law that requires voters to show ident...\n",
       "446    FILE PHOTO -- Chief Executive Officer of Unite...\n",
       "447    WASHINGTON The United States has made slight a...\n",
       "448    REFILE -- CORRECTING TYPO -- A student who was...\n",
       "449    A view of Alabama State Capital, where Alabama...\n",
       "450    U.S. Navy guided-missile destroyer USS Porter ...\n",
       "451    FILE PHOTOS: A combination of file photos show...\n",
       "452    FILE PHOTO - A U.S. F18 fighter jet lands on t...\n",
       "453    U.S. Ambassador to the United Nations Nikki Ha...\n",
       "454    PALM BEACH, Fla./WASHINGTON Top White House ai...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_articles[\"text\"].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MNB] -- Cross Validation Metrics\n",
      "Total articles classified: 1930\n",
      "Accuracy Score: 0.909\n",
      "F1 Score: 0.904\n",
      "Confusion matrix:\n",
      "[[930  35]\n",
      " [141 824]]\n",
      "\n",
      "[SVM] -- Cross Validation Metrics\n",
      "Total articles classified: 1930\n",
      "Accuracy Score: 0.947\n",
      "F1 Score: 0.946\n",
      "Confusion matrix:\n",
      "[[923  42]\n",
      " [ 61 904]]\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', ngram_range=(1,1), min_df=0, lowercase=True, stop_words='english')\n",
    "tfidf = TfidfTransformer()\n",
    "    \n",
    "#Perform cross validation\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "mnb_scores = []\n",
    "svm_scores = []\n",
    "mnb_f_scores=[]\n",
    "svm_f_scores=[]\n",
    "mnb_confusion = np.array([[0, 0], [0, 0]])\n",
    "svm_confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "for train_index, test_index in k_fold.split(even_articles):\n",
    "    train_text = even_articles.iloc[train_index]['text'].values\n",
    "    train_counts = count_vect.fit_transform(train_text)\n",
    "    train_tfidf = tfidf.fit_transform(train_counts)\n",
    "    train_y = even_articles.iloc[train_index]['label'].values\n",
    "    \n",
    "    test_text = even_articles.iloc[test_index]['text'].values\n",
    "    test_counts = count_vect.transform(test_text)\n",
    "    test_tfidf = tfidf.transform(test_counts)\n",
    "    test_y = even_articles.iloc[test_index]['label'].values\n",
    "       \n",
    "    #MNB CLASSIFIER\n",
    "    mnb_clf = MultinomialNB().fit(train_tfidf, train_y)\n",
    "    mnb_predictions = mnb_clf.predict(test_tfidf)\n",
    "\n",
    "    mnb_confusion += confusion_matrix(test_y, mnb_predictions)\n",
    "    mnb_f_score = f1_score(test_y, mnb_predictions)\n",
    "    mnb_score = accuracy_score(test_y, mnb_predictions)\n",
    "    mnb_scores.append(mnb_score)\n",
    "    mnb_f_scores.append(mnb_f_score)\n",
    "    \n",
    "    #SVM CLASSIFIER\n",
    "    svm_clf = SVC(kernel=\"linear\").fit(train_tfidf, train_y)\n",
    "    svm_predictions = svm_clf.predict(test_tfidf)\n",
    "\n",
    "    svm_confusion += confusion_matrix(test_y, svm_predictions)\n",
    "    svm_f_score = f1_score(test_y, svm_predictions)\n",
    "    svm_score = accuracy_score(test_y, svm_predictions)\n",
    "    svm_scores.append(svm_score)\n",
    "    svm_f_scores.append(svm_f_score)\n",
    "\n",
    "print('[MNB] -- Cross Validation Metrics')\n",
    "print('Total articles classified:', len(train_index) + len(test_index))\n",
    "print('Accuracy Score:', round(sum(mnb_scores)/len(mnb_scores),3))\n",
    "print('F1 Score:', round(sum(mnb_f_scores)/len(mnb_f_scores),3))\n",
    "print('Confusion matrix:')\n",
    "print(mnb_confusion)\n",
    "print()\n",
    "print('[SVM] -- Cross Validation Metrics')\n",
    "print('Total articles classified:', len(train_index) + len(test_index))\n",
    "print('Accuracy Score:', round(sum(svm_scores)/len(svm_scores),3))\n",
    "print('F1 Score:', round(sum(svm_f_scores)/len(svm_f_scores),3))\n",
    "print('Confusion matrix:')\n",
    "print(svm_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t  *Credible Features*\n",
      "\t advertisement       -2.735296\n",
      "       reuters       -2.200784\n",
      "          main       -2.110389\n",
      "            mr       -2.038752\n",
      "          skip       -1.950324\n",
      "         photo       -1.848963\n",
      "           inc       -1.736684\n",
      "    affordable       -1.681863\n",
      "        bannon       -1.584194\n",
      "       editing       -1.497722\n",
      "\n",
      "\t*Non-Credible Features*\n",
      "\t   rickrwells        2.273056\n",
      "           www        2.046357\n",
      "           com        1.855959\n",
      "         gowdy        1.853888\n",
      "      facebook        1.793129\n",
      "     rickwells        1.753753\n",
      "         https        1.707026\n",
      "            us        1.705974\n",
      "    everywhere        1.687046\n",
      "         wells        1.647254\n"
     ]
    }
   ],
   "source": [
    "#Function returns log_prob_1 - log_prob_0 for each word in corpus & sorts by max (most predictive nc feats)/min (most predictive c feats)\n",
    "def show_most_predictive_feats(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    features = pd.DataFrame()\n",
    "    features[\"words\"] = feature_names\n",
    "    features[\"log_prob_0\"] = clf.feature_log_prob_[0]\n",
    "    features[\"log_prob_1\"] = clf.feature_log_prob_[1]\n",
    "    features[\"log_prob_diff\"] = features[\"log_prob_1\"] - features[\"log_prob_0\"]\n",
    "    features = features.drop('log_prob_0', 1).drop('log_prob_1', 1)\n",
    "    features_c_sort = features.sort_values(by=[\"log_prob_diff\"])\n",
    "    features_nc_sort = features.sort_values(by=[\"log_prob_diff\"], ascending=False)\n",
    "    print(\"\\t  *Credible Features*\")\n",
    "    print(\"\\t\", features_c_sort.head(n).to_string(index=False, header=False, col_space=15))\n",
    "    print()\n",
    "    print(\"\\t*Non-Credible Features*\")\n",
    "    print(\"\\t  \", features_nc_sort.head(n).to_string(index=False, header=False, col_space=15))\n",
    "\n",
    "show_most_predictive_feats(count_vect, mnb_clf, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Filter Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove words with length <=2 from text\n",
    "def remove_shortwords(in_string):\n",
    "    out_string = in_string\n",
    "    out_words = out_string.split()\n",
    "    out_words = [word for word in out_words if len(word) > 2]\n",
    "    out_string = ' '.join(word for word in out_words)\n",
    "    return(out_string)\n",
    "\n",
    "#Remove words that shouldn't have the predictive power they are showing (regularization step only necessary for training)\n",
    "def remove_overfit_words(in_string, wordlist, sourcelist, phraselist):\n",
    "    out_string = in_string\n",
    "    for phrase in phraselist:\n",
    "        out_string = out_string.replace(phrase, '')\n",
    "    out_string = out_string.lower()\n",
    "    for source in sourcelist:\n",
    "        out_string = out_string.replace(source, '')\n",
    "    #Remove URLs\n",
    "    non_url_words = [word for word in out_string.split() if (\"www\" not in word) and (\"https\" not in word)]\n",
    "    out_words = [word for word in non_url_words if word not in wordlist]\n",
    "    out_string = ' '.join(word for word in out_words)\n",
    "    return(out_string)\n",
    "\n",
    "#Load in list of overfit words & phrases from training sources\n",
    "with open(\"text_redactions.txt\", \"r\") as infile:\n",
    "    wordlist = []\n",
    "    for line in infile:\n",
    "        wordlist.append(line.replace('\\n',''))\n",
    "\n",
    "#Generate sources list\n",
    "sources = list(set(even_articles['source']))\n",
    "sourcelist = [source.replace('-', ' ') for source in sources]\n",
    "sourcelist.append('rickrwells')\n",
    "sourcelist.append('rickwells')\n",
    "sourcelist.append('rick wells')\n",
    "sourcelist.append('wall street journal')\n",
    "sourcelist.append('gop the daily dose')\n",
    "sourcelist.append('new york times')\n",
    "sourcelist.append('washington post')\n",
    "sourcelist.append('activist post')\n",
    "sourcelist.append('wsj')\n",
    "\n",
    "#Generate indicative phrase list from training sources\n",
    "phraselist = [\"Share this:\",\n",
    "              \"by usapoliticsnow admin\",\n",
    "              \"Our Standards: The Thomson Reuters Trust Principles\",\n",
    "              \"Don't forget to follow the D.C. Clothesline on Facebook and Twitter. PLEASE help spread the word by sharing our articles on your favorite social networks.\",\n",
    "              \"Share With Your Friends On Facebook, Twitter, Everywhere\",\n",
    "              \"Thank you for reading and sharing my work –  Please look for me, Rick Wells, at http://www.facebook.com/RickRWells/ , http://www.gab.ai/RickRWells , https://plus.google.com/u/0/+RickwellsUs and on my website http://RickWells.US  – Please SUBSCRIBE in the right sidebar at RickWells.US – not dot com.  I’m also at Stop The Takeover, https://www.facebook.com/StopTheTakeover/ and please follow me on Twitter @RickRWells. Subscribe also on my YouTube Channel.\",\n",
    "              \"Like this Article? Share it!\",\n",
    "              \"Do you have information the public should know? Here are some ways you can securely send information and documents to Post journalists.\",\n",
    "              \"Share news tips with us confidentially\",\n",
    "             \"Share on Facebook\",\n",
    "             \"Tweet on Twitter\",\n",
    "             \"We encourage you to share and republish our reports, analyses, breaking news and videos (Click for details).\",\n",
    "             \"Next post\",\n",
    "             \"Previous post\",\n",
    "             \"Thank you for reading and sharing my work – Facebook is trying to starve us out of existence, having cut literally 98% of our traffic over the last year. Your shares are crucial for our survival, and we thank you. We’ve also created a presence on Gab.ai and MeWe.com, although their reach is presently much smaller, the continued abuse by Facebook of conservative voices leaves us no option. We’re remaining on Facebook for the time being, as we make the transition. Please take a look when you have a chance or if we “suddenly disappear” from Facebook as has happened to many other truth-tellers. They’ll either starve us out or take us down, one way or another, sooner or later. Now and in the future, please look for me, Rick Wells, at http://www.facebook.com/RickRWells/ , http://www.gab.ai/RickRWells , https://mewe.com/profile/rick.wells.1 and on my website http://RickWells.US – Please SUBSCRIBE in the right sidebar at RickWells.US – not dot com. I’m also at Stop The Takeover, https://www.facebook.com/StopTheTakeover/ and please follow me on Twitter @RickRWells.\"]\n",
    "\n",
    "even_articles['filtered_text'] = even_articles.apply(lambda x: remove_overfit_words(x['text'], wordlist=wordlist, sourcelist=sourcelist, phraselist=phraselist), axis=1)\n",
    "even_articles['filtered_text'] = even_articles['filtered_text'].apply(remove_shortwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435    Wilbur Ross stands after being sworn Secretary...\n",
       "436    Samsung Group chief, Jay Lee arrives the offic...\n",
       "437    refugee walks along railway tracks from the Un...\n",
       "438    Handout provided Reuters February 13, 2017, Hu...\n",
       "439    U.S. President Donald Trump addresses Joint Se...\n",
       "440    U.S. President Donald Trump looks while attend...\n",
       "441    U.S. President Donald Trump attends meeting wi...\n",
       "442    Injured people are assisted after incident Wes...\n",
       "443    WASHINGTON The Republican chairman the U.S. Ho...\n",
       "444    MILWAUKEE police officer and three other peopl...\n",
       "445    Texas law that requires voters show identifica...\n",
       "446    FILE PHOTO Chief Executive Officer United Airl...\n",
       "447    WASHINGTON The United States has made slight a...\n",
       "448    REFILE CORRECTING TYPO student who was evacuat...\n",
       "449    view Alabama State Capital, where Alabama Gove...\n",
       "450    U.S. Navy guided-missile destroyer USS Porter ...\n",
       "451    FILE PHOTOS: combination file photos show U.S....\n",
       "452    FILE PHOTO U.S. F18 fighter jet lands the deck...\n",
       "453    U.S. Ambassador the United Nations Nikki Haley...\n",
       "454    PALM BEACH, Fla./WASHINGTON Top White House ai...\n",
       "Name: filtered_text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_articles['filtered_text'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 MNB & SVM classifier on \"filtered_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MNB] -- Cross Validation Metrics\n",
      "Total articles classified: 1930\n",
      "Accuracy Score: 0.913\n",
      "F1 Score: 0.908\n",
      "Confusion matrix:\n",
      "[[933  32]\n",
      " [136 829]]\n",
      "\n",
      "[SVM-Linear] -- Cross Validation Metrics\n",
      "Total articles classified: 1930\n",
      "Accuracy Score: 0.945\n",
      "F1 Score: 0.945\n",
      "Confusion matrix:\n",
      "[[922  43]\n",
      " [ 63 902]]\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', ngram_range=(1,1), min_df=0, lowercase=True, stop_words='english')\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "#Perform cross validation\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "mnb_scores = []\n",
    "svm_scores = []\n",
    "mnb_f_scores=[]\n",
    "svm_f_scores=[]\n",
    "mnb_confusion = np.array([[0, 0], [0, 0]])\n",
    "svm_confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "for train_index, test_index in k_fold.split(even_articles):\n",
    "    train_text = even_articles.iloc[train_index]['filtered_text'].values\n",
    "    train_counts = count_vect.fit_transform(train_text)\n",
    "    train_tfidf = tfidf.fit_transform(train_counts)\n",
    "    train_y = even_articles.iloc[train_index]['label'].values\n",
    "\n",
    "    test_text = even_articles.iloc[test_index]['filtered_text'].values\n",
    "    test_counts = count_vect.transform(test_text)\n",
    "    test_tfidf = tfidf.transform(test_counts)\n",
    "    test_y = even_articles.iloc[test_index]['label'].values\n",
    "\n",
    "    #MNB CLASSIFIER\n",
    "    mnb_clf = MultinomialNB().fit(train_tfidf, train_y)\n",
    "    mnb_predictions = mnb_clf.predict(test_tfidf)\n",
    "\n",
    "    mnb_confusion += confusion_matrix(test_y, mnb_predictions)\n",
    "    mnb_f_score = f1_score(test_y, mnb_predictions)\n",
    "    mnb_score = accuracy_score(test_y, mnb_predictions)\n",
    "    mnb_scores.append(mnb_score)\n",
    "    mnb_f_scores.append(mnb_f_score)\n",
    "\n",
    "    #SVM CLASSIFIER - LINEAR KERNEL\n",
    "    svm_clf = SVC(kernel=\"linear\").fit(train_tfidf, train_y)\n",
    "    svm_predictions = svm_clf.predict(test_tfidf)\n",
    "\n",
    "    svm_confusion += confusion_matrix(test_y, svm_predictions)\n",
    "    svm_f_score = f1_score(test_y, svm_predictions)\n",
    "    svm_score = accuracy_score(test_y, svm_predictions)\n",
    "    svm_scores.append(svm_score)\n",
    "    svm_f_scores.append(svm_f_score)\n",
    "\n",
    "print('[MNB] -- Cross Validation Metrics')\n",
    "print('Total articles classified:', len(train_index) + len(test_index))\n",
    "print('Accuracy Score:', round(sum(mnb_scores)/len(mnb_scores),3))\n",
    "print('F1 Score:', round(sum(mnb_f_scores)/len(mnb_f_scores),3))\n",
    "print('Confusion matrix:')\n",
    "print(mnb_confusion)\n",
    "print()\n",
    "print('[SVM-Linear] -- Cross Validation Metrics')\n",
    "print('Total articles classified:', len(train_index) + len(test_index))\n",
    "print('Accuracy Score:', round(sum(svm_scores)/len(svm_scores),3))\n",
    "print('F1 Score:', round(sum(svm_f_scores)/len(svm_f_scores),3))\n",
    "print('Confusion matrix:')\n",
    "print(svm_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t  *Credible Features*\n",
      "\t mr       -2.253599\n",
      "         photo       -2.140069\n",
      "    affordable       -1.872928\n",
      "        bannon       -1.861103\n",
      "       editing       -1.629017\n",
      "        budget       -1.547118\n",
      "       kushner       -1.481885\n",
      "        health       -1.466663\n",
      "           gop       -1.446906\n",
      "          usat       -1.442536\n",
      "      coverage       -1.438269\n",
      "            ly       -1.434778\n",
      "      proposal       -1.423855\n",
      "     insurance       -1.414949\n",
      "     lawmakers       -1.391219\n",
      "         berry       -1.375785\n",
      "      medicaid       -1.360165\n",
      "    republican       -1.351160\n",
      "           cbo       -1.339292\n",
      "   legislation       -1.290012\n",
      "   republicans       -1.282355\n",
      "      insurers       -1.274789\n",
      "       capitol       -1.273431\n",
      "    washington       -1.266804\n",
      "          care       -1.266537\n",
      "         aides       -1.237770\n",
      "           tax       -1.237538\n",
      "          park       -1.199256\n",
      "      mulvaney       -1.191956\n",
      "         story       -1.188108\n",
      "          cuts       -1.173422\n",
      "          said       -1.172508\n",
      " congressional       -1.140111\n",
      "          uber       -1.130145\n",
      "      declined       -1.129349\n",
      "       reading       -1.129328\n",
      "       updates       -1.128010\n",
      "           aca       -1.126685\n",
      "        moscow       -1.118965\n",
      "      advisers       -1.118395\n",
      "          vote       -1.098714\n",
      "   newsletters       -1.093250\n",
      "   subscribing       -1.093250\n",
      "      overhaul       -1.088380\n",
      "         trade       -1.076272\n",
      "         rates       -1.075903\n",
      "       credits       -1.064656\n",
      "       invalid       -1.061285\n",
      "    nomination       -1.060398\n",
      "       changes       -1.057349\n",
      "        yellen       -1.055925\n",
      "     investors       -1.053123\n",
      "        reform       -1.044751\n",
      "          file       -1.044606\n",
      "          ross       -1.042720\n",
      "      clicking       -1.041780\n",
      "          sign       -1.033751\n",
      "      thursday       -1.033201\n",
      "    strategist       -1.029473\n",
      "           box       -1.028607\n",
      "      shooting       -1.021904\n",
      "    occasional       -1.014931\n",
      "          corp       -1.010402\n",
      "        verify       -1.009036\n",
      "          vice       -1.007763\n",
      "       gorsuch       -1.006571\n",
      "          palm       -1.005579\n",
      "        friday       -1.000998\n",
      "        senate       -0.998052\n",
      "        kansas       -0.996923\n",
      "   legislative       -0.994870\n",
      "        repeal       -0.994704\n",
      "    conference       -0.991685\n",
      "        senior       -0.986652\n",
      "      products       -0.986399\n",
      "     obamacare       -0.979066\n",
      "    additional       -0.978921\n",
      "      commerce       -0.973242\n",
      "        rocket       -0.969073\n",
      "  confirmation       -0.968712\n",
      "     spokesman       -0.968705\n",
      "    bernardino       -0.968032\n",
      "         beach       -0.967660\n",
      "     officials       -0.966199\n",
      "       deficit       -0.956696\n",
      "      immunity       -0.954376\n",
      "         getty       -0.952231\n",
      "         panel       -0.946265\n",
      "        monday       -0.943572\n",
      "         shark       -0.940540\n",
      "        income       -0.938697\n",
      "        speaks       -0.932921\n",
      "         votes       -0.931698\n",
      "            ms       -0.931241\n",
      "     wednesday       -0.926468\n",
      "       adviser       -0.925880\n",
      "          hill       -0.925449\n",
      "     anonymity       -0.923010\n",
      "         ernst       -0.921337\n",
      "  negotiations       -0.919398\n",
      "\n",
      "\t*Non-Credible Features*\n",
      "\t   gowdy        1.834705\n",
      "         dobbs        1.628311\n",
      "       melania        1.613435\n",
      "      liberals        1.579455\n",
      "         soros        1.509640\n",
      "        sharia        1.482148\n",
      "       hillary        1.475817\n",
      "           com        1.415675\n",
      "        shares        1.404844\n",
      "      takeover        1.387527\n",
      "    mainstream        1.356055\n",
      "         truth        1.349398\n",
      "    terrorists        1.341551\n",
      "       article        1.337055\n",
      "           vin        1.325287\n",
      "          trey        1.299130\n",
      "    turbeville        1.289904\n",
      "          isis        1.272964\n",
      "        barron        1.269867\n",
      "        mccain        1.243364\n",
      "      illegals        1.229941\n",
      "       clinton        1.227439\n",
      "       sidebar        1.218793\n",
      "          lady        1.217000\n",
      "       liberal        1.216938\n",
      "       illegal        1.200460\n",
      "        stated        1.174930\n",
      "         lynch        1.144692\n",
      "           doj        1.136149\n",
      "         islam        1.124885\n",
      "     unmasking        1.096843\n",
      "          stop        1.095966\n",
      "           dot        1.095633\n",
      "         swamp        1.093541\n",
      "       hannity        1.091930\n",
      "        stamps        1.088803\n",
      "       america        1.086291\n",
      "      michelle        1.084819\n",
      "       sharing        1.073023\n",
      "           god        1.070727\n",
      "    forfeiture        1.057790\n",
      "        armani        1.055341\n",
      "         notes        1.044793\n",
      "    absolutely        1.034098\n",
      "          asks        1.032874\n",
      "        author        1.028071\n",
      "           nsa        1.024782\n",
      "       muslims        1.021951\n",
      "       loretta        1.013292\n",
      "        aliens        1.011746\n",
      "          word        1.010623\n",
      "     globalist        1.007897\n",
      "        pundit        1.002752\n",
      "        entire        0.993144\n",
      "     yesterday        0.986125\n",
      "      religion        0.979681\n",
      " usatwentyfour        0.978057\n",
      "     sanctuary        0.976829\n",
      "         proud        0.971636\n",
      "       welfare        0.970153\n",
      "         guess        0.962888\n",
      "       klayman        0.962847\n",
      "    propaganda        0.960953\n",
      "         obama        0.954650\n",
      "          fake        0.953908\n",
      "       stating        0.951248\n",
      "          lies        0.950768\n",
      "        spying        0.943776\n",
      "          fact        0.942745\n",
      "        racist        0.941356\n",
      "         knows        0.941260\n",
      "       traitor        0.935024\n",
      "       destroy        0.934192\n",
      "       leftist        0.933912\n",
      "     regarding        0.932721\n",
      "        weiner        0.932078\n",
      "           irs        0.929848\n",
      "      leftists        0.929638\n",
      "    napolitano        0.928712\n",
      "        thanks        0.923699\n",
      "           lie        0.921982\n",
      "       society        0.917042\n",
      "       hussein        0.915443\n",
      "       bongino        0.915076\n",
      "       agorist        0.908295\n",
      "         vault        0.908275\n",
      "       sheeple        0.907374\n",
      "          hsbc        0.906265\n",
      "        expose        0.903616\n",
      "     kissinger        0.900213\n",
      "        vargas        0.899887\n",
      "           lou        0.895258\n",
      "       corrupt        0.894975\n",
      "  constitution        0.892089\n",
      "      actually        0.890652\n",
      "        tucker        0.889645\n",
      "       patriot        0.886568\n",
      "         bless        0.883419\n",
      "       carlson        0.879951\n",
      "         needs        0.874052\n"
     ]
    }
   ],
   "source": [
    "show_most_predictive_feats(count_vect, mnb_clf, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5) Perform stemming on words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Perform stemming on words\n",
    "def stem_words(in_string):\n",
    "    snowball = nltk.stem.SnowballStemmer('english')\n",
    "    out_string = in_string\n",
    "    out_words = out_string.split()\n",
    "    out_words = [snowball.stem(word) for word in out_words]\n",
    "    out_string = ' '.join(word for word in out_words)\n",
    "    return(out_string)\n",
    "\n",
    "even_articles['stem_text'] = even_articles['filtered_text'].apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "even_articles['stem_text'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', ngram_range=(1,1), min_df=0)\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "#Perform cross validation\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "mnb_scores = []\n",
    "svm_scores = []\n",
    "mnb_f_scores=[]\n",
    "svm_f_scores=[]\n",
    "mnb_confusion = np.array([[0, 0], [0, 0]])\n",
    "svm_confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "for train_index, test_index in k_fold.split(even_articles):\n",
    "    train_text = even_articles.iloc[train_index]['stem_text'].values\n",
    "    train_counts = count_vect.fit_transform(train_text)\n",
    "    train_tfidf = tfidf.fit_transform(train_counts)\n",
    "    train_y = even_articles.iloc[train_index]['label'].values\n",
    "\n",
    "    test_text = even_articles.iloc[test_index]['stem_text'].values\n",
    "    test_counts = count_vect.transform(test_text)\n",
    "    test_tfidf = tfidf.transform(test_counts)\n",
    "    test_y = even_articles.iloc[test_index]['label'].values\n",
    "\n",
    "    #MNB CLASSIFIER\n",
    "    mnb_clf = MultinomialNB().fit(train_tfidf, train_y)\n",
    "    mnb_predictions = mnb_clf.predict(test_tfidf)\n",
    "\n",
    "    mnb_confusion += confusion_matrix(test_y, mnb_predictions)\n",
    "    mnb_f_score = f1_score(test_y, mnb_predictions)\n",
    "    mnb_score = accuracy_score(test_y, mnb_predictions)\n",
    "    mnb_scores.append(mnb_score)\n",
    "    mnb_f_scores.append(mnb_f_score)\n",
    "\n",
    "    #SVM CLASSIFIER - LINEAR KERNEL\n",
    "    svm_clf = SVC(kernel=\"linear\").fit(train_tfidf, train_y)\n",
    "    svm_predictions = svm_clf.predict(test_tfidf)\n",
    "\n",
    "    svm_confusion += confusion_matrix(test_y, svm_predictions)\n",
    "    svm_f_score = f1_score(test_y, svm_predictions)\n",
    "    svm_score = accuracy_score(test_y, svm_predictions)\n",
    "    svm_scores.append(svm_score)\n",
    "    svm_f_scores.append(svm_f_score)\n",
    "\n",
    "print('[MNB] -- Cross Validation Metrics')\n",
    "print('Total articles classified:', len(train_index) + len(test_index))\n",
    "print('Accuracy Score:', round(sum(mnb_scores)/len(mnb_scores),3))\n",
    "print('F1 Score:', round(sum(mnb_f_scores)/len(mnb_f_scores),3))\n",
    "print('Confusion matrix:')\n",
    "print(mnb_confusion)\n",
    "print()\n",
    "print('[SVM-Linear] -- Cross Validation Metrics')\n",
    "print('Total articles classified:', len(train_index) + len(test_index))\n",
    "print('Accuracy Score:', round(sum(svm_scores)/len(svm_scores),3))\n",
    "print('F1 Score:', round(sum(svm_f_scores)/len(svm_f_scores),3))\n",
    "print('Confusion matrix:')\n",
    "print(svm_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_most_informative_feats(count_vect, mnb_clf, n=10)\n",
    "print()\n",
    "show_most_predictive_feats(count_vect, mnb_clf, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. \"Tonal\" Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_sentences(text):  \n",
    "    caps = \"([A-Z])\"\n",
    "    prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "    suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "    starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "    acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "    websites = \"[.](com|net|org|io|gov)\"\n",
    "    \n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "even_articles['sentences'] = even_articles['text'].apply(split_into_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_cap_punc(in_string):\n",
    "    \"\"\"Function removes capitalization and punctuation from string.\"\"\"\n",
    "    out_string = in_string\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    out_string = out_string.translate(translator)\n",
    "    out_words = out_string.split()\n",
    "    out_words = [word.lower() for word in out_words]\n",
    "    out_string = ' '.join(word for word in out_words)\n",
    "    return(out_string)\n",
    "\n",
    "def sent_analysis(text, uoa=\"sentences\"):\n",
    "    if uoa == \"sentences\":\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        counter=0\n",
    "        total_compound=0\n",
    "        for sentence in text:\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            total_compound = total_compound + ss['compound']\n",
    "            counter+=1\n",
    "\n",
    "        if counter==0:\n",
    "            avg_compound=0\n",
    "        else:\n",
    "            avg_compound = total_compound/counter\n",
    "\n",
    "        return(avg_compound)\n",
    "    \n",
    "    elif uoa == \"string\":\n",
    "        filtered_text = remove_cap_punc(text)\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        ss = sid.polarity_scores(filtered_text)\n",
    "        compound = ss['compound']\n",
    "        return(compound)\n",
    "    else:\n",
    "        print(\"uoa (unit of analysis) not recognized\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "even_articles['text_sentiment'] = even_articles['sentences'].apply(sent_analysis)\n",
    "even_articles['title_sentiment'] = even_articles.apply(lambda x: sent_analysis(x['title'], uoa=\"string\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credible avg. sentiment score on text: 0.017398579309684907 +/- 0.3187142662741009\n",
      "Non-Credible avg. sentiment score on text: -0.009672212517137782 +/- 0.3582173707414181\n",
      "\n",
      "Credible avg. sentiment score on title: -0.11036341968911922 +/- 0.7865409887338386\n",
      "Non-Credible avg. sentiment score on title: -0.16928601036269442 +/- 0.8123442940446496\n"
     ]
    }
   ],
   "source": [
    "print('Credible avg. sentiment score on text:', np.mean(even_articles['text_sentiment'][even_articles['label']==0]), '+/-', 2*np.std(even_articles['text_sentiment'][even_articles['label']==0]))\n",
    "print('Non-Credible avg. sentiment score on text:', np.mean(even_articles['text_sentiment'][even_articles['label']==1]), '+/-', 2*np.std(even_articles['text_sentiment'][even_articles['label']==1]))\n",
    "print()\n",
    "print('Credible avg. sentiment score on title:', np.mean(even_articles['title_sentiment'][even_articles['label']==0]), '+/-', 2*np.std(even_articles['title_sentiment'][even_articles['label']==0]))\n",
    "print('Non-Credible avg. sentiment score on title:', np.mean(even_articles['title_sentiment'][even_articles['label']==1]), '+/-', 2*np.std(even_articles['title_sentiment'][even_articles['label']==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Punctuation Usage - Specifically \"?\" & \"!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#% of characters in title that are \"?\" or \"!\"\n",
    "def pct_char_quesexcl(title):\n",
    "    try:\n",
    "        ques_excl = [char for char in title if char=='?' or char=='!']\n",
    "        return(len(ques_excl)/len(title))\n",
    "    except:\n",
    "        return(0)        \n",
    "\n",
    "#% of punctuation in text that is \"?\" or \"!\"\n",
    "def pct_punct_quesexcl(in_string):\n",
    "    try:\n",
    "        punct = [char for char in in_string if char in string.punctuation]\n",
    "        ques_excl = [p for p in punct if p=='?' or p=='!']\n",
    "        return(len(ques_excl)/len(punct))\n",
    "    except:\n",
    "        return(0)\n",
    "\n",
    "even_articles['pct_char_quesexcl_title'] = even_articles['title'].apply(pct_char_quesexcl)\n",
    "even_articles['pct_punc_quesexcl_text'] = even_articles['text'].apply(pct_punct_quesexcl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credible % of punctuation that is \"!\" or \"?\": 0.0062532262087285255 +/- 0.035651248116011294\n",
      "Non-Credible % of punctuation that is \"!\" or \"?\": 0.0516617517923891 +/- 0.15763173572935205\n",
      "\n",
      "Credible % of characters in title that is \"!\" or \"?\": 0.0007356131354078949 +/- 0.008465090900363343\n",
      "Non-Credible % of characters in title that is \"!\" or \"?\": 0.00550542731990129 +/- 0.02098457402226743\n"
     ]
    }
   ],
   "source": [
    "print('Credible % of punctuation that is \"!\" or \"?\":', np.mean(even_articles['pct_punc_quesexcl_text'][even_articles['label']==0]), '+/-', 2*np.std(even_articles['pct_punc_quesexcl_text'][even_articles['label']==0]))\n",
    "print('Non-Credible % of punctuation that is \"!\" or \"?\":', np.mean(even_articles['pct_punc_quesexcl_text'][even_articles['label']==1]), '+/-', 2*np.std(even_articles['pct_punc_quesexcl_text'][even_articles['label']==1]))\n",
    "print()\n",
    "print('Credible % of characters in title that is \"!\" or \"?\":', np.mean(even_articles['pct_char_quesexcl_title'][even_articles['label']==0]), '+/-', 2*np.std(even_articles['pct_char_quesexcl_title'][even_articles['label']==0]))\n",
    "print('Non-Credible % of characters in title that is \"!\" or \"?\":', np.mean(even_articles['pct_char_quesexcl_title'][even_articles['label']==1]), '+/-', 2*np.std(even_articles['pct_char_quesexcl_title'][even_articles['label']==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) % words ALL CAPS in title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pct_allcaps(title):\n",
    "    try:\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        title = title.translate(translator)\n",
    "        words = title.split()\n",
    "        all_caps = [word for word in words if word.isupper()]\n",
    "        return(len(all_caps)/len(words))\n",
    "    except:\n",
    "        return(0)        \n",
    "\n",
    "even_articles['pct_allcaps_title'] = even_articles['title'].apply(pct_allcaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credible % ALL CAPS words in title: 0.025823597542425202 +/- 0.09772454859141613\n",
      "Non-Credible % ALL CAPS words in title: 0.1225177758281094 +/- 0.353613109273369\n"
     ]
    }
   ],
   "source": [
    "print('Credible % ALL CAPS words in title:', np.mean(even_articles['pct_allcaps_title'][even_articles['label']==0]), '+/-', 2*np.std(even_articles['pct_allcaps_title'][even_articles['label']==0]))\n",
    "print('Non-Credible % ALL CAPS words in title:', np.mean(even_articles['pct_allcaps_title'][even_articles['label']==1]), '+/-', 2*np.std(even_articles['pct_allcaps_title'][even_articles['label']==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1) Logistic Regression & XGBoost classifier on derived \"tonal\" features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define function to reshape numpy array\n",
    "def reshape_array(array):\n",
    "    flipped_array = array.T\n",
    "    return(flipped_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation (LogisticRegression) Metrics\n",
      "Accuracy Score: 0.712\n",
      "F1 Score: 0.68\n",
      "Confusion matrix:\n",
      "[[783 182]\n",
      " [373 592]]\n",
      "\n",
      "Cross Validation (XGBoost) Metrics\n",
      "Accuracy Score: 0.773\n",
      "F1 Score: 0.757\n",
      "Confusion matrix:\n",
      "[[808 157]\n",
      " [281 684]]\n"
     ]
    }
   ],
   "source": [
    "#Perform cross validation for logistic regression, XGBoost\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "lr_scores=[]\n",
    "xgb_scores=[]\n",
    "lr_f_scores=[]\n",
    "xgb_f_scores=[]\n",
    "lr_confusion = np.array([[0, 0], [0, 0]])\n",
    "xgb_confusion = np.array([[0, 0], [0, 0]])\n",
    "for train_index, test_index in k_fold.split(even_articles):\n",
    "    train_x = np.array([even_articles.iloc[train_index]['pct_allcaps_title'].values,\n",
    "                        even_articles.iloc[train_index]['pct_punc_quesexcl_text'].values,\n",
    "                        even_articles.iloc[train_index]['pct_char_quesexcl_title'].values,\n",
    "                        even_articles.iloc[train_index]['text_sentiment'].values,\n",
    "                        even_articles.iloc[train_index]['title_sentiment'].values])\n",
    "    train_y = even_articles.iloc[train_index]['label'].values\n",
    "\n",
    "    test_x = np.array([even_articles.iloc[test_index]['pct_allcaps_title'].values,\n",
    "                       even_articles.iloc[test_index]['pct_punc_quesexcl_text'].values,\n",
    "                       even_articles.iloc[test_index]['pct_char_quesexcl_title'].values,\n",
    "                       even_articles.iloc[test_index]['text_sentiment'].values,\n",
    "                       even_articles.iloc[test_index]['title_sentiment'].values])\n",
    "    test_y = even_articles.iloc[test_index]['label'].values\n",
    "\n",
    "    #LOGISTIC REGRESSION\n",
    "    lr_clf = LogisticRegression().fit(reshape_array(train_x), train_y)\n",
    "    lr_predictions = lr_clf.predict(reshape_array(test_x))\n",
    "\n",
    "    lr_confusion += confusion_matrix(test_y, lr_predictions)\n",
    "    lr_f_score = f1_score(test_y, lr_predictions)\n",
    "    lr_score = accuracy_score(test_y, lr_predictions)\n",
    "    lr_scores.append(lr_score)\n",
    "    lr_f_scores.append(lr_f_score)\n",
    "\n",
    "    #XGBOOST\n",
    "    xgb_clf = XGBClassifier(max_depth=3, n_estimators=100).fit(reshape_array(train_x), train_y)\n",
    "    xgb_predictions = xgb_clf.predict(reshape_array(test_x))\n",
    "\n",
    "    xgb_confusion += confusion_matrix(test_y, xgb_predictions)\n",
    "    xgb_f_score = f1_score(test_y, xgb_predictions)\n",
    "    xgb_score = accuracy_score(test_y, xgb_predictions)\n",
    "    xgb_scores.append(xgb_score)\n",
    "    xgb_f_scores.append(xgb_f_score)\n",
    "\n",
    "\n",
    "print('Cross Validation (LogisticRegression) Metrics')\n",
    "print('Accuracy Score:', round(sum(lr_scores)/len(lr_scores),3))\n",
    "print('F1 Score:', round(sum(lr_f_scores)/len(lr_f_scores),3))\n",
    "print('Confusion matrix:')\n",
    "print(lr_confusion)\n",
    "print()\n",
    "print('Cross Validation (XGBoost) Metrics')\n",
    "print('Accuracy Score:', round(sum(xgb_scores)/len(xgb_scores),3))\n",
    "print('F1 Score:', round(sum(xgb_f_scores)/len(xgb_f_scores),3))\n",
    "print('Confusion matrix:')\n",
    "print(xgb_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation (MNB) Metrics\n",
      "Accuracy Score: 0.909\n",
      "F1 Score: 0.903\n",
      "Confusion matrix:\n",
      "[[932  33]\n",
      " [143 822]]\n",
      "\n",
      "Cross Validation (XGBoost) Metrics\n",
      "Accuracy Score: 0.775\n",
      "F1 Score: 0.758\n",
      "Confusion matrix:\n",
      "[[816 149]\n",
      " [285 680]]\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', ngram_range=(1,1), min_df=0, lowercase=True, stop_words='english')\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "mnb_scores=[]\n",
    "xgb_scores=[]\n",
    "mnb_f_scores=[]\n",
    "xgb_f_scores=[]\n",
    "mnb_confusion = np.array([[0, 0], [0, 0]])\n",
    "xgb_confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "for train_index, test_index in k_fold.split(even_articles):\n",
    "    train_text = even_articles.iloc[train_index]['filtered_text'].values\n",
    "    train_counts = count_vect.fit_transform(train_text)\n",
    "    train_tfidf = tfidf.fit_transform(train_counts)\n",
    "    train_x = np.array([even_articles.iloc[train_index]['pct_allcaps_title'].values,\n",
    "                        even_articles.iloc[train_index]['pct_punc_quesexcl_text'].values,\n",
    "                        even_articles.iloc[train_index]['pct_char_quesexcl_title'].values,\n",
    "                        even_articles.iloc[train_index]['text_sentiment'].values,\n",
    "                        even_articles.iloc[train_index]['title_sentiment'].values])\n",
    "    train_y = even_articles.iloc[train_index]['label'].values\n",
    "  \n",
    "    test_text = even_articles.iloc[test_index]['filtered_text'].values\n",
    "    test_counts = count_vect.transform(test_text)\n",
    "    test_tfidf = tfidf.transform(test_counts)\n",
    "    test_x = np.array([even_articles.iloc[test_index]['pct_allcaps_title'].values,\n",
    "                       even_articles.iloc[test_index]['pct_punc_quesexcl_text'].values,\n",
    "                       even_articles.iloc[test_index]['pct_char_quesexcl_title'].values,\n",
    "                       even_articles.iloc[test_index]['text_sentiment'].values,\n",
    "                       even_articles.iloc[test_index]['title_sentiment'].values])\n",
    "    test_y = even_articles.iloc[test_index]['label'].values\n",
    "    \n",
    "    #MNB CLASSIFIER\n",
    "    mnb_clf = MultinomialNB().fit(train_tfidf, train_y)\n",
    "    mnb_predictions = mnb_clf.predict(test_tfidf)\n",
    "    mnb_predict_probs = mnb_clf.predict_proba(test_tfidf)\n",
    "\n",
    "    mnb_confusion += confusion_matrix(test_y, mnb_predictions)\n",
    "    mnb_f_score = f1_score(test_y, mnb_predictions)\n",
    "    mnb_score = accuracy_score(test_y, mnb_predictions)\n",
    "    mnb_scores.append(mnb_score)\n",
    "    mnb_f_scores.append(mnb_f_score)\n",
    "    \n",
    "    #XGBOOST\n",
    "    xgb_clf = XGBClassifier(max_depth=3, n_estimators=100).fit(reshape_array(train_x), train_y)\n",
    "    xgb_predictions = xgb_clf.predict(reshape_array(test_x))\n",
    "    xgb_predict_probs = xgb_clf.predict_proba(reshape_array(test_x))\n",
    "\n",
    "    xgb_confusion += confusion_matrix(test_y, xgb_predictions)\n",
    "    xgb_f_score = f1_score(test_y, xgb_predictions)\n",
    "    xgb_score = accuracy_score(test_y, xgb_predictions)\n",
    "    xgb_scores.append(xgb_score)\n",
    "    xgb_f_scores.append(xgb_f_score)\n",
    "    \n",
    "print('Cross Validation (MNB) Metrics')\n",
    "print('Accuracy Score:', round(sum(mnb_scores)/len(mnb_scores),3))\n",
    "print('F1 Score:', round(sum(mnb_f_scores)/len(mnb_f_scores),3))\n",
    "print('Confusion matrix:')\n",
    "print(mnb_confusion)\n",
    "print()    \n",
    "print('Cross Validation (XGBoost) Metrics')\n",
    "print('Accuracy Score:', round(sum(xgb_scores)/len(xgb_scores),3))\n",
    "print('F1 Score:', round(sum(xgb_f_scores)/len(xgb_f_scores),3))\n",
    "print('Confusion matrix:')\n",
    "print(xgb_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Correct: 278\n",
      "MNB Correct; XGB Incorrect: 72\n",
      "MNB Incorrect; XGB Correct: 26\n",
      "Both Incorrect: 10\n",
      "\n",
      "Mean MNB Correct Prob: 0.7858433349693283\n",
      "Mean MNB Incorrect Prob: 0.6034020739141651\n",
      "Mean XGB Correct Prob: 0.8092877933461415\n",
      "Mean XGB Incorrect Prob: 0.681334788479456\n"
     ]
    }
   ],
   "source": [
    "both_correct = 0\n",
    "mnb_correct_xgb_incorrect = 0\n",
    "mnb_incorrect_xgb_correct = 0\n",
    "both_incorrect = 0\n",
    "\n",
    "mnb_correct_probs = 0\n",
    "mnb_incorrect_probs = 0\n",
    "xgb_correct_probs = 0\n",
    "xgb_incorrect_probs = 0\n",
    "\n",
    "avg_probs=[]\n",
    "weighted_probs=[]\n",
    "\n",
    "\n",
    "for mnb_pred, xgb_pred, actual, mnb_prob, xgb_prob in zip(mnb_predictions, xgb_predictions, test_y, mnb_predict_probs, xgb_predict_probs):\n",
    "    if mnb_pred == xgb_pred == actual:\n",
    "        both_correct+=1\n",
    "        mnb_correct_probs += max(mnb_prob)\n",
    "        xgb_correct_probs += max(xgb_prob)\n",
    "           \n",
    "    elif mnb_pred == actual and xgb_pred != actual:\n",
    "        mnb_correct_xgb_incorrect+=1\n",
    "        mnb_correct_probs += max(mnb_prob)\n",
    "        xgb_incorrect_probs += max(xgb_prob)\n",
    "        \n",
    "    elif mnb_pred != actual and xgb_pred == actual:\n",
    "        mnb_incorrect_xgb_correct+=1\n",
    "        mnb_incorrect_probs += max(mnb_prob)\n",
    "        xgb_correct_probs += max(xgb_prob)\n",
    "        \n",
    "    else:\n",
    "        both_incorrect+=1\n",
    "        mnb_incorrect_probs += max(mnb_prob)\n",
    "        xgb_incorrect_probs += max(xgb_prob)\n",
    "    \n",
    "    avg_probs.append((np.mean([mnb_prob[0], xgb_prob[0]]), np.mean([mnb_prob[1], xgb_prob[1]])))\n",
    "    weighted_probs.append((((0.911*mnb_prob[0])+(0.79*xgb_prob[0]))/(0.911+0.79), ((0.911*mnb_prob[1])+(0.79*xgb_prob[1]))/(0.911+0.79)))\n",
    "        \n",
    "print(\"Both Correct: {}\".format(both_correct))\n",
    "print(\"MNB Correct; XGB Incorrect: {}\".format(mnb_correct_xgb_incorrect))\n",
    "print(\"MNB Incorrect; XGB Correct: {}\".format(mnb_incorrect_xgb_correct))\n",
    "print(\"Both Incorrect: {}\".format(both_incorrect))\n",
    "print()\n",
    "print(\"Mean MNB Correct Prob: {}\".format(mnb_correct_probs/(both_correct+mnb_correct_xgb_incorrect)))\n",
    "print(\"Mean MNB Incorrect Prob: {}\".format(mnb_incorrect_probs/(both_incorrect+mnb_incorrect_xgb_correct)))\n",
    "print(\"Mean XGB Correct Prob: {}\".format(xgb_correct_probs/(both_correct+mnb_incorrect_xgb_correct)))\n",
    "print(\"Mean XGB Incorrect Prob: {}\".format(xgb_incorrect_probs/(both_incorrect+mnb_correct_xgb_incorrect)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB: 0.9067357512953368\n",
      "XGB: 0.7875647668393783\n",
      "Combined-Avg: 0.9067357512953368\n",
      "Combined-Weighted: 0.9041450777202072\n"
     ]
    }
   ],
   "source": [
    "mnb_correct = 0\n",
    "xgb_correct = 0\n",
    "avg_correct = 0\n",
    "weighted_correct = 0\n",
    "\n",
    "for mnb_pred, xgb_pred, avg_prob, weighted_prob, actual in zip(mnb_predictions, xgb_predictions, avg_probs, weighted_probs, test_y):\n",
    "    if avg_prob[0] > avg_prob[1]:\n",
    "        avg_pred = 0\n",
    "    else:\n",
    "        avg_pred=1\n",
    "    \n",
    "    if weighted_prob[0] > weighted_prob[1]:\n",
    "        weighted_pred = 0\n",
    "    else:\n",
    "        weighted_pred = 1\n",
    "        \n",
    "    if mnb_pred == actual:\n",
    "        mnb_correct+=1\n",
    "        \n",
    "    if xgb_pred == actual:\n",
    "        xgb_correct+=1\n",
    "        \n",
    "    if avg_pred == actual:\n",
    "        avg_correct+=1\n",
    "        \n",
    "    if weighted_pred == actual:\n",
    "        weighted_correct+=1\n",
    "        \n",
    "print(\"MNB: {}\".format(mnb_correct/len(test_y)))\n",
    "print(\"XGB: {}\".format(xgb_correct/len(test_y)))\n",
    "print(\"Combined-Avg: {}\".format(avg_correct/len(test_y)))\n",
    "print(\"Combined-Weighted: {}\".format(weighted_correct/len(test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.91074826134596643, 0.089251731733445902) 0.0\n",
      "(0.9409467213888596, 0.059053288992022057) 0.0\n",
      "(0.87066890587490597, 0.12933110104568227) 0.0\n",
      "(0.82555837548580313, 0.17444162451419554) 0.0\n",
      "(0.67841107842723314, 0.32158892157276425) 0.0\n",
      "(0.84693994766338609, 0.1530600523366176) 0.0\n",
      "(0.75711674479843483, 0.24288324136039346) 0.0\n",
      "(0.79588092665693488, 0.20411908718424088) 0.0\n",
      "(0.88782617246297257, 0.11217382061644417) 0.0\n",
      "(0.90547262975762866, 0.094527377162960349) 0.0\n",
      "(0.75231070406386635, 0.24768929593613292) 0.0\n",
      "(0.88258108963255666, 0.11741891728802779) 0.0\n",
      "(0.82353475253637265, 0.17646524054304374) 0.0\n",
      "(0.70507592526048979, 0.2949240747395126) 0.0\n",
      "(0.85175779488352255, 0.14824220511647251) 0.0\n",
      "(0.89457762163591314, 0.1054223714435016) 0.0\n",
      "(0.58133288183283349, 0.41866711816716584) 0.0\n",
      "(0.86919501399155852, 0.1308049790878569) 0.0\n",
      "(0.8780373110002101, 0.12196270284096383) 0.0\n",
      "(0.67422650479592683, 0.32577349520407206) 0.0\n",
      "(0.84743006212739491, 0.15256993441230979) 0.0\n",
      "(0.82447642850200009, 0.17552358533917425) 0.0\n",
      "(0.84977675424741805, 0.15022325267316586) 0.0\n",
      "(0.80275627772801306, 0.19724372227198811) 0.0\n",
      "(0.80084014359591216, 0.19915985640408687) 0.0\n",
      "(0.86473338412103906, 0.13526660203778976) 0.0\n",
      "(0.75704361347206928, 0.24295637268675788) 0.0\n",
      "(0.92164559899464915, 0.078354394084758611) 0.0\n",
      "(0.8755950523802426, 0.12440494069916891) 0.0\n",
      "(0.8107657741114116, 0.18923422588859193) 0.0\n",
      "(0.87198894463777621, 0.1280110588225159) 0.0\n",
      "(0.75577593497551276, 0.24422406502448363) 0.0\n",
      "(0.81115808136857959, 0.18884191171083545) 0.0\n",
      "(0.78624611921095255, 0.21375388078904625) 0.0\n",
      "(0.67382858150695479, 0.32617143233421519) 0.0\n",
      "(0.80866817812436564, 0.19133180803445846) 0.0\n",
      "(0.872751266895475, 0.12724873310452009) 0.0\n",
      "(0.91000302786187048, 0.089996965217540109) 0.0\n",
      "(0.82396742925467015, 0.17603256382473995) 0.0\n",
      "(0.9148422812963013, 0.085157725624279534) 0.0\n",
      "(0.83918399491807782, 0.16081601892309624) 0.0\n",
      "(0.90056105494117267, 0.099438938138244365) 0.0\n",
      "(0.87875411553569671, 0.12124587754371573) 0.0\n",
      "(0.83061540594251893, 0.16938458713689583) 0.0\n",
      "(0.68847226415051532, 0.31152772200831103) 0.0\n",
      "(0.78886969541691843, 0.21113029074190828) 0.0\n",
      "(0.64745231373304446, 0.35254767242578189) 0.0\n",
      "(0.88631986493959791, 0.1136801419809889) 0.0\n",
      "(0.922109899704603, 0.077890107215981316) 0.0\n",
      "(0.68930963047225524, 0.31069038336891586) 0.0\n",
      "(0.77836345132996743, 0.22163654867003399) 0.0\n",
      "(0.7497541499265683, 0.25024585007343253) 0.0\n",
      "(0.73078715055612919, 0.26921284944386931) 0.0\n",
      "(0.78089248606544603, 0.2191075208551386) 0.0\n",
      "(0.87273072955901199, 0.12726927044098871) 0.0\n",
      "(0.79367702176155686, 0.2063229851590298) 0.0\n",
      "(0.70758955689039316, 0.29241044310960856) 0.0\n",
      "(0.90084496899793354, 0.099155017160894157) 0.0\n",
      "(0.71707777787804228, 0.28292222212195617) 0.0\n",
      "(0.78536696723589272, 0.21463303276410667) 0.0\n",
      "(0.78624573541645792, 0.21375426458354155) 0.0\n",
      "(0.71707008654161775, 0.28292990653779715) 0.0\n",
      "(0.66755151226248388, 0.33244848773751667) 0.0\n",
      "(0.89826474684604019, 0.10173524623337338) 0.0\n",
      "(0.86571697533632685, 0.13428301774308724) 0.0\n",
      "(0.7794381105841649, 0.22056188249524999) 0.0\n",
      "(0.88475520094029592, 0.11524480598028937) 0.0\n",
      "(0.79721887669923919, 0.20278110945958935) 0.0\n",
      "(0.89580453789196801, 0.10419546902861986) 0.0\n",
      "(0.79179390028610797, 0.20820609279330568) 0.0\n",
      "(0.78250620505344004, 0.21749380186714751) 0.0\n",
      "(0.69189851178965645, 0.30810147436916752) 0.0\n",
      "(0.77560552258180515, 0.22439447741819363) 0.0\n",
      "(0.85777199684642813, 0.14222799623298665) 0.0\n",
      "(0.84849454652914513, 0.15150543962968105) 0.0\n",
      "(0.8368029931650105, 0.16319702067616235) 0.0\n",
      "(0.89832206852002705, 0.10167792801968725) 0.0\n",
      "(0.54035504051971661, 0.45964495948028444) 0.0\n",
      "(0.7233951064492391, 0.27660490739193389) 0.0\n",
      "(0.86894590601548805, 0.13105408014334149) 0.0\n",
      "(0.72579286651107666, 0.27420713348892461) 0.0\n",
      "(0.94261247739089549, 0.057387526069392902) 0.0\n",
      "(0.85145269032014692, 0.14854730967985549) 0.0\n",
      "(0.62727626021658944, 0.37272373978341139) 0.0\n",
      "(0.47791386119643442, 0.52208613880357102) 0.0\n",
      "(0.61370980402442077, 0.38629019597558129) 0.0\n",
      "(0.89574831633797936, 0.10425168366201559) 0.0\n",
      "(0.80394539225384642, 0.19605460774614905) 0.0\n",
      "(0.95725601054406362, 0.042743982535348814) 0.0\n",
      "(0.79128319306754813, 0.20871679309127675) 0.0\n",
      "(0.73973718352899176, 0.26026281647100746) 0.0\n",
      "(0.89142302600159551, 0.10857698091898479) 0.0\n",
      "(0.57498932752342058, 0.42501067247657981) 0.0\n",
      "(0.79569676128382971, 0.20430323179558244) 0.0\n",
      "(0.61320841005543147, 0.38679158994457002) 0.0\n",
      "(0.54960197500441688, 0.45039802499558274) 0.0\n",
      "(0.72812399899657676, 0.27187600100343079) 0.0\n",
      "(0.86435208524580387, 0.1356479078336173) 0.0\n",
      "(0.78499078745662054, 0.21500921946396193) 0.0\n",
      "(0.56484493567040805, 0.43515506432959811) 0.0\n",
      "(0.68721909702353989, 0.31278090297646383) 0.0\n",
      "(0.87387960809554632, 0.12612038498386013) 0.0\n",
      "(0.85971636022492948, 0.14028363977507194) 0.0\n",
      "(0.57194225062822313, 0.42805774937178265) 0.0\n",
      "(0.56622299298567791, 0.43377702085549941) 0.0\n",
      "(0.77307003855137446, 0.22692997528980288) 0.0\n",
      "(0.72575833573479298, 0.27424167810638417) 0.0\n",
      "(0.90167470716710862, 0.098325306674064653) 0.0\n",
      "(0.94185914203765408, 0.058140847581459307) 0.0\n",
      "(0.8998789364416212, 0.10012106355837898) 0.0\n",
      "(0.77914045065045368, 0.22085956319071531) 0.0\n",
      "(0.42473167083552854, 0.57526832916447168) 0.0\n",
      "(0.61394477627481325, 0.3860552237251908) 0.0\n",
      "(0.91943488339922252, 0.080565109680190566) 0.0\n",
      "(0.85937913493164331, 0.14062087198893933) 0.0\n",
      "(0.89711925990669972, 0.1028807522043237) 0.0\n",
      "(0.84721011509812327, 0.15278988490187481) 0.0\n",
      "(0.60114176867435709, 0.39885823132564169) 0.0\n",
      "(0.72192916111627203, 0.27807083888372897) 0.0\n",
      "(0.80959453993159392, 0.19040546006840564) 0.0\n",
      "(0.75261084643278542, 0.24738913972604326) 0.0\n",
      "(0.79631915015390586, 0.20368085676667722) 0.0\n",
      "(0.28988576946678613, 0.71011423053321032) 0.0\n",
      "(0.64149759850129295, 0.35850241533987698) 0.0\n",
      "(0.79280455324010191, 0.20719543983931468) 0.0\n",
      "(0.59120631699496951, 0.40879369684619887) 0.0\n",
      "(0.82375268083710806, 0.17624733300406348) 0.0\n",
      "(0.30689745100908089, 0.69310254899091628) 0.0\n",
      "(0.78435485113297121, 0.215645135025856) 0.0\n",
      "(0.85717936900513469, 0.14282063791545446) 0.0\n",
      "(0.75918783507461995, 0.24081215108420895) 0.0\n",
      "(0.65715451174874018, 0.34284548133067166) 0.0\n",
      "(0.69697467552039416, 0.3030253244796095) 0.0\n",
      "(0.5955529366102732, 0.40444705646914053) 0.0\n",
      "(0.72145820790455084, 0.27854179209545221) 0.0\n",
      "(0.84847738389355609, 0.15152260572556561) 0.0\n",
      "(0.84597815992159986, 0.15402184699898736) 0.0\n",
      "(0.72192363445433261, 0.27807636554567) 0.0\n",
      "(0.59269669904345801, 0.40730330095654349) 0.0\n",
      "(0.83161141118868853, 0.16838859573190526) 0.0\n",
      "(0.80476047135536843, 0.19523952518433935) 0.0\n",
      "(0.71474861197796924, 0.2852513811014481) 0.0\n",
      "(0.76356626049922649, 0.23643373258018999) 0.0\n",
      "(0.62147770142108827, 0.3785222985789079) 0.0\n",
      "(0.55179717309024823, 0.44820282690975061) 0.0\n",
      "(0.70442804435289841, 0.2955719556470997) 0.0\n",
      "(0.79759785729315225, 0.2024021357862629) 0.0\n",
      "(0.75141560304589872, 0.24858441079527296) 0.0\n",
      "(0.81193671569450365, 0.18806327738490974) 0.0\n",
      "(0.73650317091457751, 0.26349683600601148) 0.0\n",
      "(0.60345202838692713, 0.3965479716130767) 0.0\n",
      "(0.83549535160974853, 0.16450464146966631) 0.0\n",
      "(0.92979194292158607, 0.070208070919584309) 0.0\n",
      "(0.77207357995754844, 0.22792640620127771) 0.0\n",
      "(0.72825898031305214, 0.27174101968694409) 0.0\n",
      "(0.79767189052260334, 0.20232812331856853) 0.0\n",
      "(0.65531838300831169, 0.34468160315051394) 0.0\n",
      "(0.77239223380615896, 0.22760775235266828) 0.0\n",
      "(0.87729981799142343, 0.12270017508799459) 0.0\n",
      "(0.61961159533979637, 0.38038841850137073) 0.0\n",
      "(0.78917949551945143, 0.21082050448055051) 0.0\n",
      "(0.91093105906979166, 0.089068947850797428) 0.0\n",
      "(0.86391302853323626, 0.13608698530793892) 0.0\n",
      "(0.88789962246801934, 0.1121003775319821) 0.0\n",
      "(0.51020973897666588, 0.48979026102333895) 0.0\n",
      "(0.63495606598419374, 0.36504393401580704) 0.0\n",
      "(0.93337987523455224, 0.06662013514633143) 0.0\n",
      "(0.87966084594655547, 0.12033915405344157) 0.0\n",
      "(0.92417135940826134, 0.075828633671151599) 0.0\n",
      "(0.6627551248086152, 0.33724487519138413) 0.0\n",
      "(0.60813937302340793, 0.39186062697659857) 0.0\n",
      "(0.76690694794906078, 0.2330930520509408) 0.0\n",
      "(0.67435293822997866, 0.32564706177002523) 0.0\n",
      "(0.67773382347966982, 0.32226617652032924) 0.0\n",
      "(0.90516140584564675, 0.094838594154353653) 0.0\n",
      "(0.81597935119196763, 0.18402064880803043) 0.0\n",
      "(0.92450025292293048, 0.075499733235897759) 0.0\n",
      "(0.90584592528666275, 0.094154060872160661) 0.0\n",
      "(0.81419520160697345, 0.185804798393023) 0.0\n",
      "(0.79851675585307436, 0.20148323030575327) 0.0\n",
      "(0.828573897803632, 0.17142608835519363) 0.0\n",
      "(0.47359620387995044, 0.52640379612005184) 0.0\n",
      "(0.80774787787221203, 0.19225213596896085) 0.0\n",
      "(0.72140601136230531, 0.27859398863769597) 0.0\n",
      "(0.70955556816325949, 0.2904444318367479) 0.0\n",
      "(0.87330467484427043, 0.12669533207631659) 0.0\n",
      "(0.21211844255306647, 0.78788155744693522) 1.0\n",
      "(0.060908320137673881, 0.939091679862329) 1.0\n",
      "(0.42424549731073696, 0.57575450268926609) 1.0\n",
      "(0.21240538661665503, 0.78759461338335246) 1.0\n",
      "(0.2509098456703151, 0.74909015432968462) 1.0\n",
      "(0.10807675397837588, 0.89192324602162443) 1.0\n",
      "(0.3461632114773876, 0.65383678852261184) 1.0\n",
      "(0.09925047532278182, 0.90074952467722291) 1.0\n",
      "(0.096411239537666102, 0.90358876046233227) 1.0\n",
      "(0.28702913394620788, 0.71297086605379334) 1.0\n",
      "(0.14184605420699359, 0.85815394579300608) 1.0\n",
      "(0.47589511911975069, 0.52410488088025287) 1.0\n",
      "(0.69021223805837173, 0.30978775502104028) 1.0\n",
      "(0.49299770987529556, 0.50700229012470621) 1.0\n",
      "(0.041400583038696273, 0.95859941696130724) 1.0\n",
      "(0.17399230041873523, 0.82600769958126119) 1.0\n",
      "(0.43494879008834958, 0.56505122375282224) 1.0\n",
      "(0.48779739778825248, 0.51220260913233617) 1.0\n",
      "(0.24638410672134739, 0.75361589327865286) 1.0\n",
      "(0.17204726055298789, 0.82795273944701253) 1.0\n",
      "(0.18588785904057337, 0.81411214095942441) 1.0\n",
      "(0.18424634555824179, 0.81575365444176162) 1.0\n",
      "(0.13347331798516027, 0.86652668201484018) 1.0\n",
      "(0.24732706095291326, 0.75267293904708577) 1.0\n",
      "(0.3199657310166395, 0.68003426898336028) 1.0\n",
      "(0.23143708264372612, 0.76856291735627169) 1.0\n",
      "(0.20281226223041179, 0.79718773776958951) 1.0\n",
      "(0.58924110243715888, 0.41075889756284079) 1.0\n",
      "(0.15129513752070048, 0.84870486247929822) 1.0\n",
      "(0.2738377179543583, 0.7261622820456447) 1.0\n",
      "(0.3149272033046584, 0.68507279669534304) 1.0\n",
      "(0.2947604712471828, 0.7052395287528177) 1.0\n",
      "(0.063903173836231078, 0.9360968261637661) 1.0\n",
      "(0.60085967799551443, 0.39914032200448468) 1.0\n",
      "(0.16008623267947314, 0.83991376732052492) 1.0\n",
      "(0.25054814657473862, 0.74945185342525988) 1.0\n",
      "(0.43166830633075132, 0.56833169366924785) 1.0\n",
      "(0.43292493831239692, 0.56707506168760513) 1.0\n",
      "(0.65670346335481289, 0.34329654356577011) 1.0\n",
      "(0.40965591072575891, 0.59034407543306633) 1.0\n",
      "(0.53730807188752605, 0.46269192811247295) 1.0\n",
      "(0.17641218814492354, 0.82358781185507834) 1.0\n",
      "(0.21570954179372387, 0.78429045820627985) 1.0\n",
      "(0.18429437441149552, 0.81570562558850779) 1.0\n",
      "(0.63017031435720283, 0.36982967180162618) 1.0\n",
      "(0.46265761089840179, 0.53734238910159593) 1.0\n",
      "(0.06097361631939538, 0.93902638368060787) 1.0\n",
      "(0.51677295716681515, 0.48322704975377101) 1.0\n",
      "(0.45799125139710822, 0.54200874860289017) 1.0\n",
      "(0.61358900205548395, 0.38641099794451855) 1.0\n",
      "(0.33091584715723815, 0.66908415284276546) 1.0\n",
      "(0.28102418377760052, 0.71897581622239581) 1.0\n",
      "(0.70110097706012764, 0.29889902986045541) 1.0\n",
      "(0.22650429195210106, 0.77349570804790235) 1.0\n",
      "(0.077385313739936984, 0.92261468626006271) 1.0\n",
      "(0.28358237096772132, 0.71641762903228245) 1.0\n",
      "(0.23670344859906822, 0.76329655140093489) 1.0\n",
      "(0.065396334299565165, 0.93460366570043163) 1.0\n",
      "(0.17753992972709476, 0.82246007027290247) 1.0\n",
      "(0.24113223633409328, 0.75886776366590625) 1.0\n",
      "(0.38018158919092354, 0.61981841080907796) 1.0\n",
      "(0.25723087735125771, 0.7427691226487434) 1.0\n",
      "(0.218084814770128, 0.78191518522986703) 1.0\n",
      "(0.39688804476747153, 0.60311195523252914) 1.0\n",
      "(0.13459095930281006, 0.86540904069719138) 1.0\n",
      "(0.59523262348562467, 0.40476737651437544) 1.0\n",
      "(0.091825075025780342, 0.90817492497421837) 1.0\n",
      "(0.10147535234872462, 0.8985246476512776) 1.0\n",
      "(0.47315643679453662, 0.52684356320546244) 1.0\n",
      "(0.55966526607473754, 0.4403347477664375) 1.0\n",
      "(0.17444275580397337, 0.82555724419602672) 1.0\n",
      "(0.18114384881114401, 0.81885615118885324) 1.0\n",
      "(0.21104712105098775, 0.78895287894900612) 1.0\n",
      "(0.088956064951335151, 0.9110439350486651) 1.0\n",
      "(0.27196491722131733, 0.72803508277868412) 1.0\n",
      "(0.33026722437587608, 0.6697327756241237) 1.0\n",
      "(0.42158106988985788, 0.57841893011014323) 1.0\n",
      "(0.211936641297473, 0.78806335870252586) 1.0\n",
      "(0.39848499950512489, 0.60151501433604626) 1.0\n",
      "(0.20826808946845116, 0.79173191053154701) 1.0\n",
      "(0.62619736700003636, 0.37380261915879237) 1.0\n",
      "(0.54216324817616401, 0.45783675182383921) 1.0\n",
      "(0.41909573059207916, 0.58090426940792306) 1.0\n",
      "(0.4094679174945961, 0.59053208250540068) 1.0\n",
      "(0.41775346500170979, 0.58224653499828694) 1.0\n",
      "(0.38186458539061152, 0.61813541460939059) 1.0\n",
      "(0.19555834620372808, 0.80444165379627008) 1.0\n",
      "(0.36810230755586498, 0.63189767860296064) 1.0\n",
      "(0.15971599075514487, 0.84028400924485691) 1.0\n",
      "(0.49245391597449051, 0.50754608402551293) 1.0\n",
      "(0.066968472229984075, 0.93303152777001563) 1.0\n",
      "(0.28636575946097359, 0.71363424053902791) 1.0\n",
      "(0.36540335475509544, 0.63459664524490733) 1.0\n",
      "(0.62433371581971642, 0.3756662841802858) 1.0\n",
      "(0.15777904577952367, 0.84222095422046972) 1.0\n",
      "(0.2716009959947166, 0.72839900400527757) 1.0\n",
      "(0.29521386559688167, 0.70478613440311688) 1.0\n",
      "(0.33216736114163287, 0.66783263885836752) 1.0\n",
      "(0.35930709684386802, 0.64069290315613392) 1.0\n",
      "(0.4193377266630402, 0.58066227333696441) 1.0\n",
      "(0.28078942495204096, 0.71921057504795594) 1.0\n",
      "(0.067629468185943439, 0.93237053181405749) 1.0\n",
      "(0.083374395835816628, 0.91662560416417993) 1.0\n",
      "(0.6880584403820813, 0.31194155961791653) 1.0\n",
      "(0.69407327277002795, 0.30592671338880145) 1.0\n",
      "(0.21578569448239748, 0.78421430551760618) 1.0\n",
      "(0.44400264783775839, 0.55599735908283066) 1.0\n",
      "(0.28075607879526282, 0.71924392120473701) 1.0\n",
      "(0.45491158699736645, 0.54508841300263688) 1.0\n",
      "(0.13760025885038582, 0.86239974114961115) 1.0\n",
      "(0.42619911201686511, 0.57380088798313134) 1.0\n",
      "(0.16840796932274518, 0.83159203067725707) 1.0\n",
      "(0.17855206627295567, 0.82144793372704294) 1.0\n",
      "(0.37109871914275999, 0.6289012808572384) 1.0\n",
      "(0.6685326038247561, 0.33146738233406942) 1.0\n",
      "(0.46284598311636371, 0.53715400996304818) 1.0\n",
      "(0.22770411436171517, 0.77229588563828477) 1.0\n",
      "(0.37537024237743455, 0.6246297576225619) 1.0\n",
      "(0.53296782173234158, 0.46703216442648848) 1.0\n",
      "(0.2177389784202112, 0.78226102157978517) 1.0\n",
      "(0.10982475543366974, 0.89017524456633335) 1.0\n",
      "(0.73393290478610762, 0.2660671055947712) 1.0\n",
      "(0.34458241884277097, 0.65541758115723214) 1.0\n",
      "(0.063923508640703564, 0.93607649135929338) 1.0\n",
      "(0.28858162587961111, 0.71141837412039499) 1.0\n",
      "(0.10101264400146291, 0.8989873559985383) 1.0\n",
      "(0.16203806090848619, 0.83796193909151206) 1.0\n",
      "(0.054554741069400475, 0.94544525893059994) 1.0\n",
      "(0.053491893823597911, 0.94650810617640158) 1.0\n",
      "(0.66815054329119528, 0.33184945670880578) 1.0\n",
      "(0.39589408667251808, 0.60410589948630822) 1.0\n",
      "(0.10496877485572371, 0.8950312251442798) 1.0\n",
      "(0.18544532631754582, 0.81455467368245371) 1.0\n",
      "(0.098379548413113735, 0.90162045158688553) 1.0\n",
      "(0.19559273013991937, 0.80440726986007949) 1.0\n",
      "(0.35402502282694032, 0.64597497717305719) 1.0\n",
      "(0.39056629840137835, 0.60943370159861976) 1.0\n",
      "(0.15188241018588672, 0.8481175898141099) 1.0\n",
      "(0.19895098165591213, 0.80104901834408837) 1.0\n",
      "(0.52957268481040143, 0.47042731518960035) 1.0\n",
      "(0.64991264670657789, 0.35008733945225251) 1.0\n",
      "(0.28507501267226776, 0.71492498732772713) 1.0\n",
      "(0.57039409948345221, 0.429605914357721) 1.0\n",
      "(0.19246110536484579, 0.80753889463515438) 1.0\n",
      "(0.38272817332327563, 0.61727182667672709) 1.0\n",
      "(0.15467717484601834, 0.84532282515398149) 1.0\n",
      "(0.13371562904790205, 0.8662843709521052) 1.0\n",
      "(0.058242954997654715, 0.94175704500234303) 1.0\n",
      "(0.22252433767109842, 0.77747566232889809) 1.0\n",
      "(0.49172279224508936, 0.50827719391373727) 1.0\n",
      "(0.12336254362444168, 0.8766374563755559) 1.0\n",
      "(0.31981413678516352, 0.68018584937366311) 1.0\n",
      "(0.20087963075079665, 0.79912036924920227) 1.0\n",
      "(0.46994295128357721, 0.53005704871642023) 1.0\n",
      "(0.4048833823884278, 0.59511661761157564) 1.0\n",
      "(0.57132382944773052, 0.42867617055227192) 1.0\n",
      "(0.15585324712982696, 0.84414675287017138) 1.0\n",
      "(0.54555131169420068, 0.45444868830579888) 1.0\n",
      "(0.3088134074695596, 0.69118659253043724) 1.0\n",
      "(0.11316259699930249, 0.88683740300070035) 1.0\n",
      "(0.70898353709592199, 0.29101646290407579) 1.0\n",
      "(0.59302261084636709, 0.40697739607422034) 1.0\n",
      "(0.090612586783603288, 0.90938741321639582) 1.0\n",
      "(0.093161933881998504, 0.90683806611800422) 1.0\n",
      "(0.32337427827264353, 0.67662573556853245) 1.0\n",
      "(0.25630726554452027, 0.7436927344554779) 1.0\n",
      "(0.04616761545117732, 0.9538323845488258) 1.0\n",
      "(0.42443077217861835, 0.5755692278213852) 1.0\n",
      "(0.53823028619285496, 0.46176971380714371) 1.0\n",
      "(0.37534533523944041, 0.62465466476055587) 1.0\n",
      "(0.1114659862134105, 0.88853401378659158) 1.0\n",
      "(0.11050214454257659, 0.88949785545742188) 1.0\n",
      "(0.22650429195210106, 0.77349570804790235) 1.0\n",
      "(0.17033887379249457, 0.82966112620750876) 1.0\n",
      "(0.34292291331187041, 0.65707707284695527) 1.0\n",
      "(0.44970719331084608, 0.55029279284798105) 1.0\n",
      "(0.42306027759305337, 0.57693972240694635) 1.0\n",
      "(0.58356260152935058, 0.41643740539123481) 1.0\n",
      "(0.6689110978398346, 0.33108889523958168) 1.0\n",
      "(0.63728602783696453, 0.36271395832185943) 1.0\n",
      "(0.35596667578601165, 0.64403331037281797) 1.0\n",
      "(0.31139986021220584, 0.68860013978779155) 1.0\n",
      "(0.53403218662491447, 0.46596781337508569) 1.0\n",
      "(0.1033998451772465, 0.89660015482275146) 1.0\n",
      "(0.29620156340192805, 0.70379843659807539) 1.0\n",
      "(0.14968157789298847, 0.85031842210701003) 1.0\n",
      "(0.43432523924858624, 0.5656747607514131) 1.0\n",
      "(0.44485758277961523, 0.5551424172203846) 1.0\n",
      "(0.4961632794888281, 0.50383673435234611) 1.0\n",
      "(0.26857927556715261, 0.73142072443284967) 1.0\n",
      "(0.077254485905057957, 0.92274551409494499) 1.0\n",
      "(0.51293224241680413, 0.48706775758319754) 1.0\n",
      "(0.20253939225365591, 0.79746060774634531) 1.0\n",
      "(0.1456725072941488, 0.85432749270584762) 1.0\n",
      "(0.72683995718549665, 0.27316005665567256) 1.0\n",
      "(0.29147314353765985, 0.70852685646233748) 1.0\n",
      "(0.19218380928068809, 0.80781619071931954) 1.0\n",
      "(0.41589211436526324, 0.58410787179356516) 1.0\n",
      "(0.13139393932145627, 0.86860606067854618) 1.0\n",
      "(0.42034826740059938, 0.57965173259940339) 1.0\n"
     ]
    }
   ],
   "source": [
    "for probs, pred in zip(weighted_probs, test_y):\n",
    "    print(probs, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Check for Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cred_fp = '/ebs_volume/data/Credible/'\n",
    "ncred_fp = '/ebs_volume/data/notCredible/'\n",
    "\n",
    "articles = pd.DataFrame(columns=('label',\n",
    "                                 'text',\n",
    "                                 'title',\n",
    "                                 'date',\n",
    "                                 'source'))\n",
    "i = 0    \n",
    "for root, dirs, files in os.walk(cred_fp):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\") and 'api' not in file:\n",
    "             curr_file = os.path.join(root, file)\n",
    "             #print curr_file\n",
    "             with open(curr_file) as json_file:\n",
    "                try:\n",
    "                    data = json.load(json_file)\n",
    "                    articles.loc[i] = [0,data[\"text\"],data[\"title\"],data[\"date\"],data[\"source\"]]\n",
    "                    i+=1\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "for root, dirs, files in os.walk(ncred_fp):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\") and 'api' not in file:\n",
    "             curr_file = os.path.join(root, file)\n",
    "             #print curr_file\n",
    "             with open(curr_file) as json_file:\n",
    "                try:\n",
    "                    data = json.load(json_file)\n",
    "                    articles.loc[i] = [1,data[\"text\"],data[\"title\"],data[\"date\"],data[\"source\"]]\n",
    "                    i+=1\n",
    "                except ValueError:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove duplicate articles\n",
    "print(len(articles))\n",
    "unique_articles = articles.drop_duplicates(subset = 'text')\n",
    "print(len(unique_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove really short articles (<=200 chars)\n",
    "print(len(unique_articles))\n",
    "unique_articles = unique_articles[unique_articles[\"text\"].str.len()>200]\n",
    "print(len(unique_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_creds = unique_articles[unique_articles[\"label\"]==0.0]\n",
    "all_noncreds = unique_articles[unique_articles[\"label\"]==1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credible_sources = list(set(unique_articles[\"source\"][unique_articles[\"label\"]==0]))\n",
    "non_credible_sources = list(set(unique_articles[\"source\"][unique_articles[\"label\"]==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove sources that don't contain enough articles for testing\n",
    "credible_sources.remove('new-york-times')\n",
    "credible_sources.remove('nature')\n",
    "non_credible_sources.remove('empirenews')\n",
    "non_credible_sources.remove('darkmoon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Shuffle lists and divide in 5 equal(ish) parts\n",
    "random.shuffle(credible_sources)\n",
    "random.shuffle(non_credible_sources)\n",
    "credible_sources_array=np.array(credible_sources)\n",
    "non_credible_sources_array=np.array(non_credible_sources)\n",
    "\n",
    "credible_sources_arrays = np.split(credible_sources_array, [2, 4, 6, 8, 10])\n",
    "non_credible_sources_arrays = np.split(non_credible_sources_array, [3, 5, 7, 9, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credible_sources_arrays = credible_sources_arrays[:5]\n",
    "non_credible_sources_arrays = non_credible_sources_arrays[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores_mnb = []\n",
    "f_scores_mnb = []\n",
    "confusion_mnb = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "scores_svm = []\n",
    "f_scores_svm = []\n",
    "confusion_svm = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "scores_xgb = []\n",
    "f_scores_xgb = []\n",
    "confusion_xgb = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "i=0\n",
    "\n",
    "for cred_array in credible_sources_arrays:\n",
    "    #1) Generate Train/Test splits by source\n",
    "    cred_list = list(cred_array)    \n",
    "    holdout_creds = all_creds[all_creds[\"source\"].isin(cred_list)]\n",
    "    train_creds = all_creds[~all_creds[\"source\"].isin(cred_list)]\n",
    "    \n",
    "    for non_cred_array in non_credible_sources_arrays:\n",
    "        non_cred_list = list(non_cred_array)\n",
    "        holdout_noncreds = all_noncreds[all_noncreds[\"source\"].isin(non_cred_list)].sample(n=len(holdout_creds))\n",
    "        train_noncreds = all_noncreds[~all_noncreds[\"source\"].isin(non_cred_list)].sample(n=len(train_creds))\n",
    "        \n",
    "        train_articles = pd.concat([train_creds, train_noncreds])\n",
    "        test_articles = pd.concat([holdout_creds, holdout_noncreds])\n",
    "        \n",
    "        #2) Text preprocessing for bag of words (content-based) classifiers\n",
    "        train_articles['filtered_text'] = train_articles['text'].apply(remove_cap_punc)\n",
    "        test_articles['filtered_text'] = test_articles['text'].apply(remove_cap_punc)\n",
    "        \n",
    "        train_articles['filtered_text'] = train_articles.apply(lambda x: remove_overfit_words(x['filtered_text'], wordlist=wordlist, sourcelist=sourcelist, phraselist=phraselist), axis=1)\n",
    "        test_articles['filtered_text'] = test_articles.apply(lambda x: remove_overfit_words(x['filtered_text'], wordlist=wordlist, sourcelist=sourcelist, phraselist=phraselist), axis=1)\n",
    "        \n",
    "        train_articles['filtered_text'] = train_articles['filtered_text'].apply(remove_shortwords)\n",
    "        test_articles['filtered_text'] = test_articles['filtered_text'].apply(remove_shortwords)\n",
    "        \n",
    "        train_articles['filtered_text'] = train_articles['filtered_text'].apply(remove_stopwords)\n",
    "        train_articles['filtered_text'] = train_articles['filtered_text'].apply(remove_shortwords)\n",
    "        test_articles['filtered_text'] = test_articles['filtered_text'].apply(remove_stopwords)\n",
    "        test_articles['filtered_text'] = test_articles['filtered_text'].apply(remove_shortwords)\n",
    "        \n",
    "        #3) MNB classification\n",
    "        count_vect = CountVectorizer(analyzer='word', ngram_range=(1,1), min_df=0)\n",
    "        tfidf = TfidfTransformer()\n",
    "    \n",
    "        confusion = np.array([[0, 0], [0, 0]])\n",
    "        train_text = train_articles['filtered_text'].values\n",
    "        train_counts = count_vect.fit_transform(train_text)\n",
    "        train_tfidf = tfidf.fit_transform(train_counts)\n",
    "        train_y = train_articles['label'].values\n",
    "\n",
    "        test_text = test_articles['filtered_text'].values\n",
    "        test_counts = count_vect.transform(test_text)\n",
    "        test_tfidf = tfidf.transform(test_counts)\n",
    "        test_y = test_articles['label'].values\n",
    "\n",
    "        mnb_clf = MultinomialNB().fit(train_tfidf, train_y)\n",
    "        mnb_predictions = mnb_clf.predict(test_tfidf)\n",
    "\n",
    "        confusion_mnb += confusion_matrix(test_y, mnb_predictions)\n",
    "        f_score_mnb = f1_score(test_y, mnb_predictions)\n",
    "        score_mnb = accuracy_score(test_y, mnb_predictions)\n",
    "        scores_mnb.append(score_mnb)\n",
    "        f_scores_mnb.append(f_score_mnb)\n",
    "        \n",
    "        print(\"[MNB] -- Test on {0} & {1}\".format(cred_list, non_cred_list))\n",
    "        print('Total articles classified:', len(mnb_predictions))\n",
    "        print('Accuracy Score:', round(score_mnb, 3))\n",
    "        print('F1 Score:', round(f_score_mnb, 3))\n",
    "        print('Confusion matrix:')\n",
    "        print(confusion_matrix(test_y, mnb_predictions))\n",
    "        print()\n",
    "        \n",
    "        #4) Linear SVM Classification\n",
    "        svm_clf = SVC(kernel=\"linear\").fit(train_tfidf, train_y)\n",
    "        svm_predictions = svm_clf.predict(test_tfidf)\n",
    "\n",
    "        confusion_svm += confusion_matrix(test_y, svm_predictions)\n",
    "        f_score_svm = f1_score(test_y, svm_predictions)\n",
    "        score_svm = accuracy_score(test_y, svm_predictions)\n",
    "        scores_svm.append(score_svm)\n",
    "        f_scores_svm.append(f_score_svm)\n",
    "        \n",
    "        print(\"[SVM] -- Test on {0} & {1}\".format(cred_list, non_cred_list))\n",
    "        print('Total articles classified:', len(svm_predictions))\n",
    "        print('Accuracy Score:', round(score_svm, 3))\n",
    "        print('F1 Score:', round(f_score_svm, 3))\n",
    "        print('Confusion matrix:')\n",
    "        print(confusion_matrix(test_y, svm_predictions))\n",
    "        print()\n",
    "        \n",
    "        #5) Text preprocessing for tone-based classification\n",
    "        train_articles['sentences'] = train_articles['text'].apply(split_into_sentences)\n",
    "        train_articles['text_sentiment'] = train_articles['sentences'].apply(sent_analysis)\n",
    "        train_articles['title_sentiment'] = train_articles.apply(lambda x: sent_analysis(x['title'], uoa=\"string\"), axis=1)\n",
    "        test_articles['sentences'] = test_articles['text'].apply(split_into_sentences)\n",
    "        test_articles['text_sentiment'] = test_articles['sentences'].apply(sent_analysis)\n",
    "        test_articles['title_sentiment'] = test_articles.apply(lambda x: sent_analysis(x['title'], uoa=\"string\"), axis=1)\n",
    "        \n",
    "        train_articles['pct_char_quesexcl_title'] = train_articles['title'].apply(pct_char_quesexcl)\n",
    "        train_articles['pct_punc_quesexcl_text'] = train_articles['text'].apply(pct_punct_quesexcl)\n",
    "        test_articles['pct_char_quesexcl_title'] = test_articles['title'].apply(pct_char_quesexcl)\n",
    "        test_articles['pct_punc_quesexcl_text'] = test_articles['text'].apply(pct_punct_quesexcl)\n",
    "        \n",
    "        train_articles['pct_allcaps_title'] = train_articles['title'].apply(pct_allcaps)\n",
    "        test_articles['pct_allcaps_title'] = test_articles['title'].apply(pct_allcaps)\n",
    "        \n",
    "        #6) XGBoost Classification\n",
    "        xgb_train_x = np.array([train_articles['pct_allcaps_title'].values,\n",
    "                                train_articles['pct_punc_quesexcl_text'].values,\n",
    "                                train_articles['pct_char_quesexcl_title'].values,\n",
    "                                train_articles['text_sentiment'].values,\n",
    "                                train_articles['title_sentiment'].values])\n",
    "        xgb_train_y = train_articles['label'].values\n",
    "        \n",
    "        xgb_test_x = np.array([test_articles['pct_allcaps_title'].values,\n",
    "                               test_articles['pct_punc_quesexcl_text'].values,\n",
    "                               test_articles['pct_char_quesexcl_title'].values,\n",
    "                               test_articles['text_sentiment'].values,\n",
    "                               test_articles['title_sentiment'].values])\n",
    "        xgb_test_y = test_articles['label'].values\n",
    "        \n",
    "        xgb_clf = XGBClassifier(max_depth=3, n_estimators=100).fit(reshape_array(xgb_train_x), xgb_train_y)\n",
    "        xgb_predictions = xgb_clf.predict(reshape_array(xgb_test_x))\n",
    "\n",
    "        confusion_xgb += confusion_matrix(xgb_test_y, xgb_predictions)\n",
    "        f_score_xgb = f1_score(xgb_test_y, xgb_predictions)\n",
    "        score_xgb = accuracy_score(xgb_test_y, xgb_predictions)\n",
    "        scores_xgb.append(score_xgb)\n",
    "        f_scores_xgb.append(f_score_xgb)\n",
    "        \n",
    "        print(\"[XGB] -- Test on {0} & {1}\".format(cred_list, non_cred_list))\n",
    "        print('Total articles classified:', len(xgb_predictions))\n",
    "        print('Accuracy Score:', round(score_xgb, 3))\n",
    "        print('F1 Score:', round(f_score_xgb, 3))\n",
    "        print('Confusion matrix:')\n",
    "        print(confusion_matrix(xgb_test_y, xgb_predictions))\n",
    "        print()\n",
    "                \n",
    "        i+=1\n",
    "        print(\"COMPLETED {0}/{1} ITERATIONS\".format(i,len(credible_sources_arrays)*len(non_credible_sources_arrays)))\n",
    "        print()\n",
    "        \n",
    "print(\"*---------------------------*\")        \n",
    "print('GENERALIZATION (MNB) Metrics')\n",
    "print('Accuracy Score:', round(sum(scores_mnb)/len(scores_mnb),3))\n",
    "print('F1 Score:', round(sum(f_scores_mnb)/len(f_scores_mnb),3))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_mnb)\n",
    "print()\n",
    "\n",
    "print('GENERALIZATION (SVM) Metrics')\n",
    "print('Accuracy Score:', round(sum(scores_svm)/len(scores_svm),3))\n",
    "print('F1 Score:', round(sum(f_scores_svm)/len(f_scores_svm),3))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_svm) \n",
    "print()\n",
    "\n",
    "print('GENERALIZATION (XGB) Metrics')\n",
    "print('Accuracy Score:', round(sum(scores_xgb)/len(scores_xgb),3))\n",
    "print('F1 Score:', round(sum(f_scores_xgb)/len(f_scores_xgb),3))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pizza_gate_text = \"\"\"Comet Pizza is a pizza place owned by James Alefantis, who is the former gay boyfriend of David Brock, the CEO of Correct The Record. It has been the venue for dozens of events for the Hillary campaign staff. John Podesta has had campaign fundraisers there for both Barack Obama and Hillary Clinton. John’s brother and business partner Tony Podesta has his birthday party there every year. [https://i.sli.mg/1MqPHA.png]\n",
    "\n",
    "It’s also a dive that according to reviews and photos has hidden bathroom doors and creepy murals. The bathrooms in particular have murals exclusively of nude women, as well as a great deal of graffiti relating to sex. Reviews of the restaurant are bizarrely polarized. Websites describing it positively note that there are regularly “unsupervised children running around”. Their menu include a pedophilic symbol, as do the signs and decorations of other neighboring businesses.\n",
    "\n",
    "The music acts and the posters promoting same acts are bizarre in their presentation, content, and lyrical focus, but are still promoted as being “for all ages”. The overtly sexual content would suggest otherwise.\n",
    "\n",
    "The same has taken place in reference to videos recorded inside Comet Ping Pong by people that frequent their establishment as well as video referencing Comet Ping Pong positively from the exterior.\n",
    "\n",
    "While initially not the central focus of the investigation at the onset, Comet Ping Pong is a much more overt and much more disturbing hub of coincidences. Everyone associated with the business is making semi-overt, semi-tongue-in-cheek, and semi-sarcastic inferences towards sex with minors. The artists that work for and with the business also generate nothing but cultish imagery of disembodiment, blood, beheadings, sex, and of course pizza.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SINGLE EXAMPLE\n",
    "\n",
    "print(\"original_text: {0} words, {1} characters\".format(len(pizza_gate_text.split()), len(pizza_gate_text)))\n",
    "print()\n",
    "print(pizza_gate_text)\n",
    "\n",
    "pizza_gate_filtered_text = remove_cap_punc(pizza_gate_text)\n",
    "print()\n",
    "print(\"Removed cap/punc: {0} words, {1} characters\".format(len(pizza_gate_filtered_text.split()), len(pizza_gate_filtered_text)))\n",
    "print()\n",
    "print(pizza_gate_filtered_text)\n",
    "       \n",
    "pizza_gate_filtered_text = remove_overfit_words(pizza_gate_filtered_text, wordlist=wordlist, sourcelist=sourcelist, phraselist=phraselist)\n",
    "print()\n",
    "print(\"Removed overfit words/phrases: {0} words, {1} characters\".format(len(pizza_gate_filtered_text.split()), len(pizza_gate_filtered_text)))\n",
    "print()\n",
    "print(pizza_gate_filtered_text)\n",
    "\n",
    "pizza_gate_filtered_text = remove_shortwords(pizza_gate_filtered_text)\n",
    "print()\n",
    "print(\"Removed short words: {0} words, {1} characters\".format(len(pizza_gate_filtered_text.split()), len(pizza_gate_filtered_text)))\n",
    "print()\n",
    "print(pizza_gate_filtered_text)\n",
    "\n",
    "pizza_gate_filtered_text = remove_stopwords(pizza_gate_filtered_text)\n",
    "pizza_gate_filtered_text = remove_shortwords(pizza_gate_filtered_text)\n",
    "print()\n",
    "print(\"Removed stop words: {0} words, {1} characters\".format(len(pizza_gate_filtered_text.split()), len(pizza_gate_filtered_text)))\n",
    "print()\n",
    "print(pizza_gate_filtered_text)\n",
    "\n",
    "pizza_gate_filtered_text = stem_words(pizza_gate_filtered_text)\n",
    "print()\n",
    "print(\"Removed stop words: {0} words, {1} characters\".format(len(pizza_gate_filtered_text.split()), len(pizza_gate_filtered_text)))\n",
    "print()\n",
    "print(pizza_gate_filtered_text)\n",
    "\n",
    "print()\n",
    "print(\"Lexical Diversity: {}\".format(lexical_diversity(pizza_gate_filtered_text)))\n",
    "print(\"Punctuation Analysis: {}\".format(pct_punct_quesexcl(pizza_gate_text)))\n",
    "\n",
    "print(clf.predict(np.array([lexical_diversity(pizza_gate_filtered_text), pct_punct_quesexcl(pizza_gate_text)]).reshape(1,-1)))\n",
    "print(clf.predict_proba(np.array([lexical_diversity(pizza_gate_filtered_text), pct_punct_quesexcl(pizza_gate_text)]).reshape(1,-1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
